
# Time Series Analysis Pitfalls - Detailed Examples

---

## 1. Ignoring Non-Stationarity

### When Things Go Wrong

A non-stationary series with a trend is used directly in an ARIMA model, leading to:
- Poor parameter estimates.
- Misleading predictions.

**Example**:
```python
import numpy as np
from statsmodels.tsa.arima.model import ARIMA

# Simulated non-stationary data (cumulative sum of random noise)
data = np.cumsum(np.random.randn(100))

# Fit ARIMA without differencing
model = ARIMA(data, order=(1, 0, 0))  # No differencing
fit = model.fit()

print(f"Predictions: {fit.forecast(steps=5)}")
```

### Problem:
The predictions will be completely off because the model assumes the data is stationary, but itâ€™s not.

### How to Fix:
Transform the data to make it stationary using **differencing** or **log transformation**.

**Example**:
```python
# Difference the data to remove trend
stationary_data = np.diff(data)

model = ARIMA(stationary_data, order=(1, 0, 0))  # Now on stationary data
fit = model.fit()
print(f"Fixed Predictions: {fit.forecast(steps=5)}")
```

---

## 2. Overfitting the Model

### When Things Go Wrong

Using too many lags in an ARIMA model creates overfitting, leading to:
- Extremely low training error.
- Poor performance on unseen data.

**Example**:
```python
from statsmodels.tsa.arima.model import ARIMA

# Simulated data
data = np.random.randn(100)

# Overfitted ARIMA model with too many parameters
model = ARIMA(data, order=(5, 0, 5))  # Excessive lags
fit = model.fit()

print(f"AIC: {fit.aic}, Parameters: {fit.params}")
```

### Problem:
The AIC will be artificially low, but the model won't generalize to test data.

### How to Fix:
Use **AIC** or **BIC** to select optimal parameters.

**Example**:
```python
# Optimal parameter selection using AIC
model = ARIMA(data, order=(2, 0, 2))  # Reduced lags
fit = model.fit()
print(f"Optimized AIC: {fit.aic}")
```

---

## 3. Misinterpreting Correlations

### When Things Go Wrong

A high correlation is found between two variables, but it is due to a shared trend rather than a causal relationship.

**Example**:
```python
import pandas as pd
import numpy as np

# Simulate two independent time series with a shared trend
x = np.cumsum(np.random.randn(100) + 0.1)
y = np.cumsum(np.random.randn(100) + 0.1)
data = pd.DataFrame({'x': x, 'y': y})

print(f"Correlation: {data.corr()}")
```

### Problem:
The correlation is high (close to 1), but the variables are not causally related. The trend drives the correlation.

### How to Fix:
Detrend the data before calculating correlation or use Granger causality to test for relationships.

**Example**:
```python
# Detrend the data
data_detrended = data.diff().dropna()

print(f"Detrended Correlation: {data_detrended.corr()}")
```

---

## 4. Failing to Account for Seasonality

### When Things Go Wrong

A model ignores seasonal patterns, leading to systematic errors in forecasts.

**Example**:
```python
import pandas as pd
from statsmodels.tsa.holtwinters import ExponentialSmoothing

# Simulated seasonal data
data = pd.Series(np.sin(np.linspace(0, 4 * np.pi, 100)) + np.random.normal(scale=0.1, size=100))

# Fit a simple Exponential Smoothing model (ignoring seasonality)
model = ExponentialSmoothing(data, seasonal=None)
fit = model.fit()

# Forecast
forecast = fit.forecast(steps=10)
print(f"Forecast: {forecast}")
```

### Problem:
The forecasts fail to capture the repeating seasonal pattern, resulting in poor accuracy.

### How to Fix:
Use seasonal decomposition or models that explicitly account for seasonality (e.g., SARIMA, Holt-Winters).

**Example**:
```python
# Fit a seasonal Exponential Smoothing model
model_seasonal = ExponentialSmoothing(data, seasonal='add', seasonal_periods=12)
fit_seasonal = model_seasonal.fit()

forecast_seasonal = fit_seasonal.forecast(steps=10)
print(f"Seasonal Forecast: {forecast_seasonal}")
```

---

## 5. Overlooking Residual Analysis

### When Things Go Wrong

Residuals are autocorrelated or show patterns, indicating model misspecification.

**Example**:
```python
import matplotlib.pyplot as plt

# Simulated data with autocorrelation
data = np.sin(np.linspace(0, 10, 100)) + np.random.normal(scale=0.1, size=100)
model = ARIMA(data, order=(1, 0, 0))
fit = model.fit()

# Plot residuals
residuals = fit.resid
plt.plot(residuals)
plt.axhline(0, color='red', linestyle='--')
plt.title("Residuals")
plt.show()
```

### Problem:
The residuals show systematic patterns, indicating the model failed to capture key dynamics.

### How to Fix:
Refine the model (e.g., add seasonal terms or adjust differencing).

---

## 6. Not Scaling or Transforming Data

### When Things Go Wrong

Input data is not scaled, causing poor convergence or incorrect weights in machine learning models.

**Example**:
```python
from sklearn.linear_model import LinearRegression

# Simulated unscaled data
X = np.array([100, 200, 300, 400]).reshape(-1, 1)
y = np.array([10, 20, 30, 40])

# Linear regression without scaling
model = LinearRegression()
model.fit(X, y)
print(f"Coefficients: {model.coef_}")
```

### Problem:
Large input values (X) dominate the model, leading to poorly generalized coefficients.

### How to Fix:
Scale the input data.

**Example**:
```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

model.fit(X_scaled, y)
print(f"Scaled Coefficients: {model.coef_}")
```

---

## 7. Overlooking Feature Engineering in Multivariate Models

### When Things Go Wrong

Multivariate models miss critical relationships due to lack of lagged features or derived statistics.

**Example**:
```python
import pandas as pd

# Simulated multivariate data
data = pd.DataFrame({'var1': np.random.rand(100), 'var2': np.random.rand(100)})

# Fit a model without lagged features
from statsmodels.tsa.api import VAR
model = VAR(data)
fit = model.fit(maxlags=1)
print(f"Coefficients: {fit.params}")
```

### Problem:
The model doesn't capture lagged dependencies or interactions between variables.

### How to Fix:
Add lagged features and rolling statistics.

**Example**:
```python
data['var1_lag1'] = data['var1'].shift(1)
data['var2_rolling_mean'] = data['var2'].rolling(window=5).mean()
data = data.dropna()

model = VAR(data)
fit = model.fit(maxlags=1)
print(f"Improved Coefficients: {fit.params}")
```
